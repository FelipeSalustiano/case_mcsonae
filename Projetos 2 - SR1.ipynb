{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPosjvJI7sHtpr1Qk77kJIb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# PROJETO MC SONAE\n","\n","## Introdução:\n","\n","A MC Sonae é um conglomerado português com presença em diversos países, atuando em múltiplos setores da economia, como varejo, imobiliário, tecnologia, entre outros.\n","\n","Apesar de sua ampla atuação e estrutura organizacional robusta, a empresa enfrenta desafios significativos na gestão de dados entre as diferentes unidades do grupo. Atualmente, os processos de coleta e consolidação de informações são majoritariamente manuais, descentralizados e despadronizados. Essa ausência de uma governança de dados eficaz compromete a consistência das informações, dificultando o planejamento estratégico, a tomada de decisões e a alocação eficiente de investimentos.\n","\n","## Objetivo:\n","\n","O principal objetivo do projeto é o desenvolvimento de um sistema de data scraping capaz de coletar, estruturar e padronizar os dados provenientes das diversas áreas e empresas que compõem o grupo MC Sonae.\n","\n","Esse processo visa:\n","\n","*   Eliminar a fragmentação e a redundância de dados;\n","*   Padronizar formatos e estruturas informacionais;\n","*   Criar uma base de dados unificada, segura e confiável.\n","\n","A partir dessa infraestrutura consolidada, serão gerados dashboards dinâmicos e interativos, facilitando a visualização e análise dos dados. Esses painéis visuais permitirão aos gestores maior agilidade na interpretação das informações, subsidiando decisões estratégicas baseadas em dados precisos e atualizados.\n","\n","## Premissas:\n","\n","Para o desenvolvimento do projeto, foram adotadas algumas premissas:\n","\n","\n","*   Que os arquivo a serem lidos e tratados estão semiestruturados, ou seja, eles já são fornecidos com alguma estrutura. Para a correta leitura desses arquivos, essa estrutura precisa ser respeitada;\n","\n","\n","*   O sistema é capaz de ler arquivos nos seguintes formatos: PDF, DOCX, CSV e XLSX. Para os arquivos PDF e DOCX, deve haver apenas dados em forma de texto. Para o formato XLSX e CSV, deve haver dados tabulares (tabelas);\n","\n","\n","*   Os arquivos tabulares devem já estar formatados com as seguintes colunas: Empresa, Ano, Receita Total (receita bruta), Lucro Líquido, Custo Operacional (OPEX), Número de Funcionários, País e Setor;\n","\n","## Estrutura do Projeto\n","\n","O processo de integração de dados consiste em coleta de informações de múltiplas fontes, transformações desses dados para um formato mais adequado e carregamento para um destino final para análise posterior.\n","\n","As múltiplas fontes de informações são os arquivos de vários formatos que devem estar na pasta `entrada`. Esses dados serão lidos e tratados. Desse processo, serão gerados arquivos gráficos e arquivos provenientes de tratamento de texto e tabela. Os arquivos gráficos serão salvos na pasta `saida_graficos`, com os outros arquivos sendo gravados na pasta `saida`.\n","\n","Por existirem várias etapas, optou-se por utilizar a metodologia funcional. As várias etapas do processo são divididas em módulos, com cada módulo possuindo funções específicas que executam apenas uma única tarefa específica. Dessa forma, o projeto foi estruturado da seguinte forma:\n"],"metadata":{"id":"c4lg3VPD9VGg"}},{"cell_type":"markdown","source":["### `main.py`\n","\n","Este é \"cérebro\" de todo o processo. Sua função é gerenciar o fluxo de trabalho, decidir o que precisa ser feito com cada arquivo e chamar os outros módulos para executar as tarefas.\n","\n","\n","\n","*   Função `processar_arquivo_tabela()`: executa a sequência completa de operações para arquivos de tabela (.csv e .xlsx).\n","\n","\n","1.   Chama `extrair_tabela` para ler o arquivo e obter um DataFrame;\n","\n","2.   Passa o DataFrame bruto para o `pipeline_tratamento` para limpeza e padronização;\n","\n","1.   Usa o DataFrame tratado para chamar `gerar_todos_graficos`;\n","\n","2.   Chama `salvar_tabela_como_csv` para salvar o resultado em um novo arquivo `.csv`;\n","\n","1.   Por fim, chama `sumario_executivo` para exibir o resumo dos dados no terminal."],"metadata":{"id":"r57xW2wYkLC5"}},{"cell_type":"markdown","source":["*   Função `processar_arquivo_texto()`: executa a sequência completa de operações para arquivos de texto (.pdf e .docx).\n","\n","1.  Chama `extrair_texto` para obter uma lista de parágrafos do arquivo;\n","\n","2.  Aplica a `função limpar_texto` a cada parágrafo;\n","\n","1.  Agrega os parágrafos limpos para chamar `gerar_estatisticas_texto`.\n","\n","2.  Monta um dicionário final com as estatísticas e os parágrafos limpos;\n","\n","1.  Chama `salvar_texto_como_json` para salvar este dicionário em um arquivo `.json`."],"metadata":{"id":"31e3P1c476VV"}},{"cell_type":"markdown","source":["\n","\n","*   Função `main()`: é a função que inicia tudo.\n","\n","\n","1.   Define e cria as pastas `entrada` e `saida`;\n","2.   Usa `os.listdir()` para obter uma lista com os nomes de todos os arquivos na pasta entrada;\n","\n","\n","1.   Inicia um laço `for` que percorre cada arquivo da lista;\n","2.   Dentro do laço, verifica a extensão do arquivo e decide se deve chamar `processar_arquivo_tabela` ou `processar_arquivo_texto`.\n","\n","\n","\n","\n","\n"],"metadata":{"id":"j5vTIqpZ79yT"}},{"cell_type":"code","source":["\"\"\"\n","Módulo Principal\n","\n","Este script é o ponto de entrada do programa. Ele é responsável por:\n","1. Definir as pastas de entrada e saída de forma segura, baseando-se na localização do próprio script\n","2. Escanear a pasta de entrada em busca de arquivos para processar\n","3. Para cada arquivo encontrado, determinar o tipo de processamento necessário com base na sua extensão\n","4. Chamar as funções de processamento específicas para tabelas (.csv, .xlsx) ou para textos (.pdf, .docx)\n","\"\"\"\n","\n","import os\n","from manipulacao_arquivo import extrair_tabela, extrair_texto\n","from tratamento_dados import pipeline_tratamento\n","from tratamento_texto import limpar_texto, gerar_estatisticas_texto\n","from salvar_dados import salvar_tabela_como_csv, salvar_texto_como_json\n","from sumario import sumario_executivo\n","from grafico import gerar_todos_graficos\n","\n","CAMINHO_BASE_DO_SCRIPT = os.path.dirname(os.path.abspath(__file__))\n","PASTA_ENTRADA = os.path.join(CAMINHO_BASE_DO_SCRIPT, \"entrada\")\n","PASTA_SAIDA = os.path.join(CAMINHO_BASE_DO_SCRIPT, \"saida\")\n","\n","\n","def processar_arquivo_tabela(caminho_arquivo: str, nome_arquivo: str):\n","    \"\"\"\n","    Executa a pipeline completa para arquivos de tabela (CSV, XLSX).\n","    \"\"\"\n","    # Etapa 1: Extrair os dados brutos da tabela do arquivo\n","    dado_bruto = extrair_tabela(caminho_arquivo)\n","\n","    # Se a extração falhar, retorna None e interrompe a função\n","    if dado_bruto is None:\n","        return\n","\n","    # Etapa 2: Aplicar todo o pipeline de tratamento e limpeza dos dados\n","    dado_tratado = pipeline_tratamento(dado_bruto)\n","\n","    # Se o tratamento falhar, interrompe a função\n","    if dado_tratado is None:\n","        return\n","\n","    # Etapa 3: Gera e salva os gráficos com base nos dados tratados\n","    gerar_todos_graficos(dado_tratado)\n","\n","    # Etapa 4: Preparar o nome e o caminho do arquivo de saída\n","    nome_base, _ = os.path.splitext(nome_arquivo)\n","    nome_saida_csv = f\"{nome_base}_tratado.csv\"\n","    caminho_saida_csv = os.path.join(PASTA_SAIDA, nome_saida_csv)\n","\n","    # Etapa 5: Salvar o DataFrame tratado no novo arquivo CSV\n","    salvar_tabela_como_csv(dado_tratado, caminho_saida_csv)\n","\n","    # Imprime o sumário dos indicadores financeiros\n","    sumario_executivo(dado_tratado)\n","\n","\n","def processar_arquivo_texto(caminho_arquivo: str, nome_arquivo: str):\n","    \"\"\"\n","    Executa a pipeline completa para arquivos de texto (PDF, DOCX)\n","    \"\"\"\n","\n","    # Etapa 1: Extrair o texto do arquivo como uma lista de parágrafos\n","    lista_paragrafos_brutos = extrair_texto(caminho_arquivo)\n","\n","    if not lista_paragrafos_brutos:\n","        print(\"   - Falha ao extrair texto ou arquivo sem conteúdo. Pulando para o próximo arquivo.\")\n","        return\n","\n","    # Etapa 2: Limpar cada parágrafo individualmente\n","    print(\"   - Limpando cada parágrafo extraído...\")\n","    paragrafos_limpos = []\n","    for p in lista_paragrafos_brutos:\n","        paragrafo_processado = limpar_texto(p)\n","        paragrafos_limpos.append(paragrafo_processado)\n","\n","    paragrafos_realmente_limpos = []\n","    for p in paragrafos_limpos:\n","        if p:\n","            paragrafos_realmente_limpos.append(p)\n","    paragrafos_limpos = paragrafos_realmente_limpos\n","\n","    # Etapa 3: Gerar estatísticas a partir do texto completo\n","    print(\"   - Gerando estatísticas gerais do texto...\")\n","    texto_completo_limpo = \" \".join(paragrafos_limpos)\n","    estatisticas = gerar_estatisticas_texto(texto_completo_limpo)\n","\n","    # Etapa 4: Montar a estrutura do dicionário para salvar no JSON\n","    dados_finais = {\n","        \"estatisticas_gerais\": estatisticas,\n","        \"paragrafos_limpos\": paragrafos_limpos\n","    }\n","\n","    # Etapa 5: Preparar o nome e o caminho do arquivo de saída JSON\n","    nome_base, _ = os.path.splitext(nome_arquivo)\n","    nome_saida_json = f\"{nome_base}_texto.json\"\n","    caminho_saida_json = os.path.join(PASTA_SAIDA, nome_saida_json)\n","\n","    # Etapa 6: Salvar o dicionário final no novo arquivo JSON\n","    salvar_texto_como_json(dados_finais, caminho_saida_json)\n","\n","def main():\n","    \"\"\"\n","    Função principal que inicia e gerencia todo o processo\n","    \"\"\"\n","    print(\"=\"*60)\n","    print(\" \" * 20 + \"INICIANDO PROCESSAMENTO\")\n","    print(\"=\"*60)\n","\n","    os.makedirs(PASTA_ENTRADA, exist_ok=True)\n","    os.makedirs(PASTA_SAIDA, exist_ok=True)\n","\n","    try:\n","        lista_arquivos = os.listdir(PASTA_ENTRADA)\n","        if not lista_arquivos:\n","            print(f\"\\nAVISO: A pasta '{PASTA_ENTRADA}' está vazia.\")\n","            return\n","\n","    except FileNotFoundError:\n","        print(f\"ERRO: A pasta de entrada '{PASTA_ENTRADA}' não foi encontrada!\")\n","        return\n","\n","    print(f\"\\nEncontrados {len(lista_arquivos)} arquivo(s) na pasta de entrada.\")\n","\n","    for nome_arquivo in lista_arquivos:\n","        caminho_completo = os.path.join(PASTA_ENTRADA, nome_arquivo)\n","        _, extensao = os.path.splitext(nome_arquivo)\n","        extensao = extensao.lower()\n","\n","        print(f\"\\n--- Processando arquivo: '{nome_arquivo}' ---\")\n","\n","        if extensao in ['.csv', '.xlsx']:\n","            processar_arquivo_tabela(caminho_completo, nome_arquivo)\n","\n","        elif extensao in ['.pdf', '.docx']:\n","            processar_arquivo_texto(caminho_completo, nome_arquivo)\n","\n","        else:\n","            print(f\"   - AVISO: Extensão '{extensao}' não suportada.\")\n","\n","    print(\"\\n\" + \"=\"*60)\n","    print(\" \" * 17 + \"PROCESSAMENTO FINALIZADO\")\n","    print(\"=\"*60)\n","\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"qfYTc3pKm9Ng"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### `manipulacao_arquivo.py`\n","\n","Este módulo é responsável pela primeira etapa: Extrair (o \"E\" de ETL). Ele sabe como abrir e ler os diferentes formatos de arquivo."],"metadata":{"id":"quF3IabKm92h"}},{"cell_type":"markdown","source":["\n","\n","*   Função `extrair_tabela()`: lê um arquivo `.csv` ou `.xlsx` e o converte em uma tabela de dados (DataFrame). Utiliza a biblioteca pandas `pd.read_csv` e `pd.read_excel`, que é a ferramenta padrão em Python para manipulação de tabelas.\n"],"metadata":{"id":"VyeniGzu9WNO"}},{"cell_type":"markdown","source":["\n","\n","*   Função `extrair_texto()`: lê um arquivo `.pdf` ou `.docx` e extrai todo o seu conteúdo textual, retornando-o como uma lista de parágrafos. Para `.pdf`, usa a biblioteca pdfplumber para extrair o texto de cada página e depois o divide por quebras de linha. Para `.docx`, usa a biblioteca `python-docx` que já consegue identificar e extrair cada parágrafo individualmente.\n","\n"],"metadata":{"id":"ZEAeHQEK9-g1"}},{"cell_type":"code","source":["\"\"\"\n","Módulo de Manipulação de Arquivos (Extração)\n","\n","Este módulo contém as funções responsáveis por ler os diferentes tipos de arquivos\n","(.csv, .xlsx, .pdf, .docx) e extrair seu conteúdo bruto.\n","Ele separa a lógica de extração de tabelas e de textos em funções diferentes.\n","\"\"\"\n","\n","import os\n","import pdfplumber\n","import pandas as pd\n","from docx import Document\n","\n","def extrair_tabela(caminho_arquivo: str) -> pd.DataFrame:\n","    \"\"\"\n","    Extrai uma tabela de arquivos .csv ou .xlsx\n","    \"\"\"\n","    print(f\"   - Tentando extrair TABELA de '{os.path.basename(caminho_arquivo)}'...\")\n","\n","    # Extrai a extensão do arquivo para determinar como lê-lo\n","    _, extensao = os.path.splitext(caminho_arquivo)\n","    extensao = extensao.lower()\n","\n","    # Bloco try/except para capturar possíveis erros durante a leitura do arquivo\n","    try:\n","        if extensao == '.csv':\n","            # Se for CSV, usa a função read_csv do Pandas.\n","            df = pd.read_csv(caminho_arquivo)\n","        elif extensao == '.xlsx':\n","            # Se for Excel, usa a função read_excel do Pandas.\n","            df = pd.read_excel(caminho_arquivo)\n","        else:\n","            # Se a extensão não for suportada para tabelas, informa e retorna None.\n","            print(f\"   - ERRO: Extensão '{extensao}' não é suportada para extração de tabelas.\")\n","            return None\n","\n","        print(\"     -> Tabela extraída com sucesso.\")\n","        # Retorna o DataFrame criado.\n","        return df\n","\n","    except Exception as e:\n","        # Se qualquer outro erro ocorrer, imprime o erro e retorna None\n","        print(f\"   - ERRO ao ler o arquivo de tabela: {e}\")\n","        return None\n","\n","def extrair_texto(caminho_arquivo: str) -> list[str]:\n","    \"\"\"\n","    Extrai o texto corrido de arquivos .pdf ou .docx, parágrafo por parágrafo\n","    \"\"\"\n","    print(f\"   - Tentando extrair TEXTO de '{os.path.basename(caminho_arquivo)}'...\")\n","\n","    _, extensao = os.path.splitext(caminho_arquivo)\n","    extensao = extensao.lower()\n","    lista_paragrafos = [] # Inicializa uma lista vazia para armazenar os parágrafos.\n","\n","    try:\n","        if extensao == '.pdf':\n","            # Usa a biblioteca pdfplumber para abrir e ler o PDF.\n","            with pdfplumber.open(caminho_arquivo) as pdf:\n","                # Corre cada página do documento.\n","                for pagina in pdf.pages:\n","                    texto_pagina = pagina.extract_text()\n","                    # Se algum texto for extraído da página\n","                    if texto_pagina:\n","                        # divide o texto da página por quebras de linha ('\\n') para simular parágrafos\n","                        paragrafos_pagina = texto_pagina.split('\\n')\n","                        # Adiciona os \"parágrafos\" encontrados à lista principal\n","                        lista_paragrafos.extend(paragrafos_pagina)\n","\n","        elif extensao == '.docx':\n","            # Usa a biblioteca python-docx para abrir o documento Word\n","            documento = Document(caminho_arquivo)\n","            # A biblioteca já fornece uma lista de parágrafos\n","            for paragrafo in documento.paragraphs:\n","                # Adiciona o texto de cada parágrafo à lista\n","                lista_paragrafos.append(paragrafo.text)\n","\n","        else:\n","            # Se a extensão não for suportada para textos, informa e retorna None\n","            print(f\"   - ERRO: Extensão '{extensao}' não é suportada para extração de texto.\")\n","            return None\n","\n","        # Limpa a lista, removendo itens vazios ou que contêm apenas espaços\n","        paragrafos_filtrados = [] # Cria uma lista vazia para o resultado\n","        for p in lista_paragrafos: # Corre cada parágrafo extraído\n","            # A condição 'if p' verifica se a string não é vazia.\n","            # A condição 'not p.isspace()' verifica se a string não contém apenas espaços em branco\n","            if p and not p.isspace():\n","                # Remove espaços em branco do início e do fim do parágrafo\n","                paragrafos_filtrados.append(p.strip())\n","\n","\n","        # Se a lista final de parágrafos não estiver vazia\n","        if paragrafos_filtrados:\n","            print(f\"     -> Texto extraído com sucesso. {len(paragrafos_filtrados)} parágrafos encontrados.\")\n","            # Retorna a lista de parágrafos limpos.\n","            return paragrafos_filtrados\n","        else:\n","            # Se, após a filtragem, a lista estiver vazia, informa e retorna None\n","            print(\"   - AVISO: Nenhum texto foi encontrado no arquivo.\")\n","            return None\n","\n","    except Exception as e:\n","        # Se qualquer erro ocorrer durante o processo, imprime a mensagem e retorna None.\n","        print(f\"   - ERRO ao extrair texto do arquivo: {e}\")\n","        return None"],"metadata":{"id":"SqVt8FrHoZRO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### `tratamento_dados.py`\n","\n","Este módulo é responsável pela segunda etapa para dados tabulares: Transformar (o \"T\" de ETL). Sua missão é pegar a tabela bruta e deixá-la limpa, padronizada e pronta para análise.\n","\n","\n","\n","*   Função `remover_duplicadas()`: remove linhas que são cópias exatas de outras;\n","\n","\n","*   Função `converter_tipos_colunas()`: garante que colunas que deveriam ser numéricas (como 'Ano', 'Lucro Líquido') sejam de fato tratadas como números, e não como texto;\n","*   Função `tratar_dados_nulos()`: encontra células vazias na tabela e as preenche com valores padrão (0 para números, 'Não Informado' para textos);\n","\n","\n","*   Função `padronizar_texto()`: padroniza textos para manter a consistência.\n","*   Função `pipeline_tratamento()`: orquestra todas as funções de tratamento, executando as funções de limpeza na sequência correta.\n"],"metadata":{"id":"ob3nG6V4oiRf"}},{"cell_type":"code","source":["\"\"\"\n","Módulo de Tratamento de Dados de Tabela\n","\n","Este módulo contém todas as funções necessárias para limpar e transformar os dados\n","extraídos em formato de tabela (DataFrame do Pandas).\n","As operações incluem remoção de duplicatas, conversão de tipos,\n","tratamento de valores nulos e padronização de textos.\n","\"\"\"\n","\n","import pandas as pd\n","\n","def remover_duplicadas(dados: pd.DataFrame) -> pd.DataFrame:\n","    \"\"\"\n","    Verifica e remove linhas duplicadas de um DataFrame.\n","    \"\"\"\n","    # Conta o número de linhas que são duplicatas exatas.\n","    num_duplicatas = dados.duplicated().sum()\n","\n","    # Se encontrar uma ou mais duplicatas\n","    if num_duplicatas > 0:\n","        print(f\"\\n---Removendo {num_duplicatas} linhas duplicadas ...\")\n","        # Remove as duplicatas, mantendo a primeira ocorrência e rearruma o índice do DataFrame após a remoção\n","        dados_sem_duplicatas = dados.drop_duplicates(keep='first').reset_index(drop=True)\n","        return dados_sem_duplicatas\n","    # Caso contrário\n","    else:\n","        print(\"\\n---Não foram encontradas linhas duplicadas.\")\n","        return dados\n","\n","def converter_tipos_colunas(dados: pd.DataFrame) -> pd.DataFrame:\n","    \"\"\"\n","    Converte colunas específicas para o tipo numérico.\n","    \"\"\"\n","    print(\"\\n---Convertendo tipos de dados das colunas ...\")\n","\n","    # Lista de colunas que esperamos que sejam numéricas\n","    colunas_numericas = ['Ano', 'Receita Total (receita bruta)', 'Lucro Líquido',\n","                         'Custo Operacional (OPEX)', 'Número de Funcionários']\n","\n","    # Varr cada nome de coluna na lista.\n","    for coluna in colunas_numericas:\n","        # Verifica se a coluna realmente existe no DataFrame.\n","        if coluna in dados.columns:\n","            # Converte a coluna para tipo numérico\n","            # 'errors=coerce' força valores que não podem ser convertidos a se tornarem 'NaN' (Not a Number)\n","            dados[coluna] = pd.to_numeric(dados[coluna], errors='coerce')\n","        else:\n","            # Se a coluna não for encontrada, imprime um aviso\n","            print(f\"\\n---Coluna numérica '{coluna}' não encontrada\")\n","\n","    return dados\n","\n","def tratar_dados_nulos(dados: pd.DataFrame) -> pd.DataFrame:\n","    \"\"\"\n","    Preenche valores nulos (NaN) em colunas numéricas e de texto\n","    \"\"\"\n","    print(\"\\n---Tratando dados nulos/em branco ...\")\n","\n","    # Para colunas numéricas, preenche os valores nulos com 0\n","    colunas_numericas = dados.select_dtypes(include=['number']).columns\n","    dados[colunas_numericas] = dados[colunas_numericas].fillna(0)\n","\n","    # Para colunas de texto, preenche os valores nulos com 'Não Informado'\n","    colunas_texto = dados.select_dtypes(include=['object']).columns\n","    dados[colunas_texto] = dados[colunas_texto].fillna('Não Informado')\n","\n","    # Garante que colunas que devem ser inteiras (sem casas decimais) sejam convertidas\n","    colunas_inteiras = ['Ano', 'Número de Funcionários']\n","    for coluna in colunas_inteiras:\n","        if coluna in dados.columns:\n","            dados[coluna] = dados[coluna].astype(int)\n","\n","    return dados\n","\n","def padronizar_texto(dados: pd.DataFrame) -> pd.DataFrame:\n","    \"\"\"\n","    Padroniza o texto de colunas específicas para o formato \"Title Case\" - Primeira letra maiúscula e o resto minúscula\n","    \"\"\"\n","    print(\"\\n---Padronizando colunas de texto ...\")\n","\n","    # Lista de colunas de texto a serem padronizadas.\n","    colunas_para_padronizar = ['Empresa', 'País', 'Setor']\n","    for coluna in colunas_para_padronizar:\n","        dados[coluna] = dados[coluna].str.title()\n","\n","    return dados\n","\n","def pipeline_tratamento(dados: pd.DataFrame) -> pd.DataFrame:\n","    \"\"\"\n","    Orquestra a execução de todas as funções de tratamento em sequência.\n","    \"\"\"\n","    # Caso de um DataFrame vazio seja passado\n","    if dados is None:\n","        return None\n","\n","    print(\"\\n---Iniciando pipeline de tratamento de dados---\")\n","\n","    # Executa cada etapa do tratamento na ordem definida.\n","    dados_tratados = converter_tipos_colunas(dados)\n","    dados_tratados = tratar_dados_nulos(dados_tratados)\n","    dados_tratados = padronizar_texto(dados_tratados)\n","    dados_tratados = remover_duplicadas(dados_tratados)\n","\n","    # Retorna o DataFrame final.\n","    return dados_tratados"],"metadata":{"id":"2VUXXLjSrGXc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### `tratamento_texto.py`\n","\n","Este módulo também faz a etapa de Transformação, mas com foco nos textos extraídos dos PDFs e DOCX.\n","\n","\n","\n","\n","\n","*   Função `limpar_texto()`: pega um parágrafo de texto e realiza uma limpeza básica.Converte tudo para minúsculas e remove sinais de pontuação para facilitar a análise computacional.\n","*   Função `gerar_estatisticas_texto()`: calcula métricas simples sobre o texto. Conta o número total de palavras, o número de palavras únicas e usa a classe `Counter` para encontrar as 5 palavras mais comuns no documento.\n","\n"],"metadata":{"id":"V_GQd2ZwrNx1"}},{"cell_type":"code","source":["\"\"\"\n","Módulo de Tratamento de Texto\n","\n","Este módulo contém funções para processamento e análise de texto.\n","As operações incluem limpeza (remoção de pontuação, normalização de caixa)\n","e a geração de estatísticas básicas, como contagem de palavras.\n","\"\"\"\n","\n","import re\n","from collections import Counter\n","import string\n","\n","def limpar_texto(texto: str) -> str:\n","    \"\"\"\n","    Realiza uma limpeza básica no texto.\n","    \"\"\"\n","    # Etapa 1: Converte toda a string para letras minúsculas\n","    texto = texto.lower()\n","\n","    # Etapa 2: Remove todos os caracteres de pontuação.\n","    # 'string.punctuation' é uma string que contém todos os sinais de pontuação comuns.\n","    # O método 'translate' é uma forma eficiente de remover múltiplos caracteres de uma vez.\n","    texto = texto.translate(str.maketrans('', '', string.punctuation))\n","\n","    # Etapa 3: Normaliza os espaços em branco.\n","    # 'texto.split()' quebra a string em uma lista de palavras (removendo espaços extras)\n","    # \"' '.join(...)\" junta a lista de volta em uma string, com um único espaço entre as palavras\n","    texto = ' '.join(texto.split())\n","\n","    return texto\n","\n","def gerar_estatisticas_texto(texto_limpo: str) -> dict:\n","    \"\"\"\n","    Gera um dicionário com estatísticas básicas sobre o texto\n","    \"\"\"\n","    # Quebra a string de texto em uma lista de palavras\n","    palavras = texto_limpo.split()\n","\n","    # Calcula o número total de palavras na lista\n","    total_palavras = len(palavras)\n","\n","    # 'set(palavras)' cria um conjunto, que automaticamente remove duplicatas.\n","    # 'len()' então nos dá o número de palavras únicas.\n","    palavras_unicas = len(set(palavras))\n","\n","    # 'Counter' é uma classe especializada que conta a frequência de cada item em uma lista\n","    frequencia_palavras = Counter(palavras)\n","    # '.most_common(5)' retorna uma lista de tuplas com as 5 palavras mais frequentes e suas contagens\n","    top_5_palavras = frequencia_palavras.most_common(5)\n","\n","    # Monta o dicionário final com os resultados\n","    estatisticas = {\n","        \"total_de_palavras\": total_palavras,\n","        \"total_de_palavras_unicas\": palavras_unicas,\n","        \"top_5_palavras_mais_comuns\": top_5_palavras\n","    }\n","\n","    return estatisticas"],"metadata":{"id":"1iOQpNXjskji"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### `salvar_dados.py`\n","\n","Este módulo é responsável pela etapa final: Carregar (o \"L\" de ETL). Ele pega os dados já processados e os salva em disco.\n","\n","\n","*   Função `salvar_tabela_como_csv()`: pega o DataFrame tratado e o salva em um novo arquivo no formato `.csv`.\n","*   Função `salvar_texto_como_json()`: pega o dicionário com as estatísticas e os parágrafos limpos e o salva em um arquivo no formato `.json`.\n","\n"],"metadata":{"id":"xBLf8oHoB-9U"}},{"cell_type":"code","source":["\"\"\"\n","Módulo de Salvamento de Dados (Carregamento)\n","\n","Este módulo contém as funções para persistir os dados processados em disco.\n","Ele lida com o salvamento de dados em forma de tabela, no formato .csv, e dados de texto, no formato .json\n","\"\"\"\n","\n","import pandas as pd\n","import json\n","import os\n","\n","def salvar_tabela_como_csv(dados: pd.DataFrame, caminho_saida: str):\n","    \"\"\"\n","    Salva um DataFrame em um arquivo no formato .csv\n","    \"\"\"\n","    # Verifica se os dados estiverem vazios\n","    if dados is None:\n","        print(\"   - AVISO: Nenhum dado de tabela para salvar.\")\n","        return\n","\n","    print(f\"   - Salvando tabela tratada em '{caminho_saida}'...\")\n","    try:\n","        # Garante que a pasta de destino exista antes de tentar salvar.\n","        os.makedirs(os.path.dirname(caminho_saida), exist_ok=True)\n","\n","        # 'index=False' impede o Pandas de salvar o índice do DataFrame como uma coluna no CSV\n","        # 'encoding='utf-8-sig'' garante a compatibilidade com acentos e caracteres especiais ao abrir o arquivo no Excel\n","        dados.to_csv(caminho_saida, index=False, encoding='utf-8-sig')\n","        print(f\"     -> Tabela salva com sucesso.\")\n","\n","    except Exception as e:\n","        print(f\"   - ERRO ao salvar o arquivo CSV: {e}\")\n","\n","\n","def salvar_texto_como_json(dados_para_salvar: dict, caminho_saida: str):\n","    \"\"\"\n","    Salva um dicionário de dados em um arquivo no formato .json.\n","    \"\"\"\n","    # Não faz nada se o dicionário estiver vazio\n","    if not dados_para_salvar:\n","        print(\"   - AVISO: Nenhum dado de texto para salvar.\")\n","        return\n","\n","    print(f\"   - Salvando dados de texto em '{caminho_saida}'...\")\n","    try:\n","        # Garante a existência do diretório\n","        os.makedirs(os.path.dirname(caminho_saida), exist_ok=True)\n","\n","        # Abre o arquivo em modo de escrita ('w') com a codificação UTF-8.\n","        with open(caminho_saida, 'w', encoding='utf-8') as f:\n","            # 'json.dump' escreve o dicionário no arquivo.\n","            # 'ensure_ascii=False' permite que caracteres acentuados sejam salvos corretamente.\n","            # 'indent=4' formata o JSON de forma legível, com 4 espaços de indentação.\n","            json.dump(dados_para_salvar, f, ensure_ascii=False, indent=4)\n","        print(f\"     -> Arquivo JSON salvo com sucesso.\")\n","\n","    except Exception as e:\n","        print(f\"   - ERRO ao salvar o arquivo JSON: {e}\")"],"metadata":{"id":"Gs-ouDHnCu2V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### `grafico.py`\n","\n","Este módulo é focado em criar representações visuais dos dados das tabelas para facilitar a compreensão. É uma função genérica que cria um único gráfico de barras e o salva como uma imagem `.png`, utilizando a biblioteca `matplotlib` para desenhar o gráfico, customizar títulos, eixos e cores, e finalmente salvá-lo em um arquivo.\n","\n","\n","\n","*   Funções `gerar_todos_graficos()`: orquestra a criação de todos os gráficos de análise do projeto. Prepara os dados (agrupando por empresa e por país) e chama a função `criar_e_salvar_grafico_barras` várias vezes, uma para cada gráfico que se deseja criar."],"metadata":{"id":"_CyTSPv0sq0L"}},{"cell_type":"code","source":["\"\"\"\n","Módulo de Geração de Gráficos\n","\n","Este módulo utiliza a biblioteca Matplotlib para criar visualizações dos dados\n","tabulares tratados. As funções aqui geram gráficos de barras e os salvam\n","como arquivos de imagem .png.\n","\"\"\"\n","\n","import os\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","def criar_e_salvar_grafico_barras(dados: pd.DataFrame, eixo_x: str, eixo_y: str, titulo: str, pasta_saida: str):\n","    \"\"\"\n","    Cria e salva um único gráfico de barras como um arquivo de imagem (.png)\n","    \"\"\"\n","    # Se não houver dados, não faz nada.\n","    if dados is None or dados.empty:\n","        print(f\"   - Aviso: Não há dados para gerar o gráfico '{titulo}'.\")\n","        return\n","\n","    print(f\"   - Gerando e salvando gráfico: '{titulo}'...\")\n","\n","    try:\n","        # Garante que a pasta de saída para os gráficos exista\n","        os.makedirs(pasta_saida, exist_ok=True)\n","\n","        # Ordena os dados pelo eixo Y em ordem decrescente para um melhor visual\n","        dados_ordenados = dados.sort_values(by=eixo_y, ascending=False)\n","\n","        # Cria uma nova figura/gráfico com um tamanho específico\n","        plt.figure(figsize=(12, 7))\n","\n","        # Plota os dados como um gráfico de barras\n","        plt.bar(dados_ordenados[eixo_x], dados_ordenados[eixo_y])\n","\n","        # Define os textos do gráfico (título, rótulos dos eixos)\n","        plt.title(titulo, fontsize=16)\n","        plt.xlabel(eixo_x, fontsize=12)\n","        plt.ylabel(eixo_y, fontsize=12)\n","\n","        # Rotaciona os rótulos do eixo X para evitar sobreposição\n","        plt.xticks(rotation=45, ha='right')\n","\n","        # Adiciona uma grade horizontal para facilitar a leitura\n","        plt.grid(axis='y', linestyle='--', alpha=0.7)\n","\n","        # Ajusta o layout para garantir que nada seja cortado\n","        plt.tight_layout()\n","\n","        # Cria um nome de arquivo a partir do título do gráfico\n","        nome_arquivo = f\"{titulo.lower().replace(' ', '_')}.png\"\n","        caminho_completo = os.path.join(pasta_saida, nome_arquivo)\n","\n","        # Salva a figura gerada no caminho especificado\n","        plt.savefig(caminho_completo)\n","\n","        # Fecha a figura salva\n","        plt.close()\n","\n","        print(f\"     -> Gráfico salvo em '{caminho_completo}'\")\n","\n","    except Exception as e:\n","        print(f\"   - ERRO ao gerar o gráfico '{titulo}': {e}\")\n","\n","\n","def gerar_todos_graficos(dados: pd.DataFrame):\n","    \"\"\"\n","    Orquestra a criação de todos os gráficos de análise definidos.\n","    \"\"\"\n","    # Se não houver dados, interrompe o processo\n","    if dados is None:\n","        print(\"\\n--- Análise gráfica interrompida: DataFrame está vazio. ---\")\n","        return\n","\n","    # Define o caminho para a pasta de saída dos gráficos de forma segura\n","    CAMINHO_BASE_DO_SCRIPT = os.path.dirname(os.path.abspath(__file__))\n","    PASTA_GRAFICOS = os.path.join(CAMINHO_BASE_DO_SCRIPT, \"saida_graficos\")\n","\n","    print(f\"\\n--- Iniciando geração de gráficos (salvando em '{PASTA_GRAFICOS}') ---\")\n","\n","    # Agrupa os dados por empresa e soma os valores numéricos\n","    dados_por_empresa = dados.groupby('Empresa').sum(numeric_only=True).reset_index()\n","\n","    # Chama a função de criação de gráficos para cada análise de empresa\n","    criar_e_salvar_grafico_barras(dados_por_empresa, 'Empresa', 'Receita Total (receita bruta)', 'Receita Total por Empresa', PASTA_GRAFICOS)\n","    criar_e_salvar_grafico_barras(dados_por_empresa, 'Empresa', 'Lucro Líquido', 'Lucro Líquido por Empresa', PASTA_GRAFICOS)\n","\n","    # Agrupa os dados por país e soma os valores numéricos\n","    dados_por_pais = dados.groupby('País').sum(numeric_only=True).reset_index()\n","\n","    # Chama a função de criação de gráficos para cada análise de país\n","    criar_e_salvar_grafico_barras(dados_por_pais, 'País', 'Receita Total (receita bruta)', 'Receita Total por País', PASTA_GRAFICOS)\n","    criar_e_salvar_grafico_barras(dados_por_pais, 'País', 'Lucro Líquido', 'Lucro Líquido por País', PASTA_GRAFICOS)\n","\n","    print(\"--- Geração de gráficos finalizada ---\")"],"metadata":{"id":"SyJZGzqitmMF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### sumario.py\n","\n","Este módulo tem o objetivo de apresentar os resultados financeiros da análise das tabelas de forma rápida e direta no terminal do usuário.\n","\n","\n","\n","*   Função `sumario_executivo()`: calcula totais e outros indicadores-chave e os exibe em um formato legível. Soma os valores de colunas financeiras importantes (como 'Receita Total' e 'Lucro Líquido') e conta quantas empresas únicas foram analisadas, imprimindo tudo em um bloco de texto bem formatado.\n","\n"],"metadata":{"id":"KRMftJquDqgM"}},{"cell_type":"code","source":["\"\"\"\n","Módulo de Geração de Sumário Executivo\n","\n","Este módulo é responsável por calcular e exibir um resumo dos principais\n","indicadores financeiros e operacionais das tabelas tratadas\n","\"\"\"\n","\n","import pandas as pd\n","\n","def sumario_executivo(dados: pd.DataFrame):\n","    \"\"\"\n","    Calcula e imprime no console um sumário dos dados.\n","    \"\"\"\n","    # Se não houver dados, imprime uma mensagem e encerra\n","    if dados is None or dados.empty:\n","        print(\"\\n--- Não há dados para gerar o sumário. ---\")\n","        return\n","\n","    # Imprime um cabeçalho para o sumário\n","    print(\"\\n\" + \"=\"*50)\n","    print(\" \" * 15 + \"SUMÁRIO EXECUTIVO\")\n","    print(\"=\"*50)\n","\n","    try:\n","        # Lista de colunas financeiras que queremos somar.\n","        colunas_financeiras = [\n","            'Receita Total (receita bruta)',\n","            'Lucro Líquido'\n","        ]\n","\n","        # Verifica se uma coluna esperada (colunas_financeiras) existe no arquivo\n","        colunas_existentes = []\n","        for col in colunas_financeiras:\n","            if col in dados.columns:\n","                colunas_existentes.append(col)\n","\n","        # Calcula a soma apenas para as colunas que foram encontradas\n","        somas = dados[colunas_existentes].sum()\n","\n","        # Conta o número de valores únicos na coluna 'Empresa'\n","        num_empresas = dados['Empresa'].nunique()\n","\n","        # Imprime o resumo dos resultados.\n","        print(f\"Indicadores Consolidados para {num_empresas} empresa(s):\")\n","\n","        # Corre os resultados das somas para exibi-los.\n","        for nome_coluna, total in somas.items():\n","            # Formatação da string para alinhar os valores e formatar os números.\n","            # ':<30' alinha o texto à esquerda em um espaço de 30 caracteres.\n","            # ':,.2f' formata o número com separador de milhar e duas casas decimais\n","            print(f\"  - {nome_coluna:<30}: € {total:,.2f}\")\n","\n","    except Exception as e:\n","        # Se ocorrer qualquer erro durante os cálculos, ele será capturado e exibido.\n","        print(f\"\\n--- ERRO ao gerar o sumário executivo: {e} ---\")\n","\n","    finally:\n","        # Este bloco é sempre executado, imprimindo o rodapé do sumário.\n","        print(\"=\"*50)"],"metadata":{"id":"3xe_GGBXEIsj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"aj1Uz7qduXxb"}}]}